{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef8f22c2-aa8b-498b-8dfd-856b0600559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch as tr\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1dd1bb-c5d4-4b4f-a8b4-b60ebdb1f9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile 'tiny_nerf_data.npz' with keys: images, poses, focal"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input images, poses, and intrinsics\n",
    "data = np.load(\"tiny_nerf_data.npz\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c548dd4-2b8e-40e8-be73-3efb851ce1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to(images, height, width):\n",
    "    # images: (B, old_H, old_W, C)\n",
    "\n",
    "    # (B, C, old_H, old_W)\n",
    "    images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "    transform = tv.transforms.Compose([\n",
    "        tv.transforms.Resize((height, width))\n",
    "    ])\n",
    "\n",
    "    # (B, C, new_H, new_W)\n",
    "    resized_images = torch.stack([\n",
    "        # (C, new_H, new_W)\n",
    "        transform(image)\n",
    "        for image in images\n",
    "    ])\n",
    "\n",
    "    # (B, new_H, new_W, C)\n",
    "    resized_images = resized_images.permute(0, 2, 3, 1)\n",
    "\n",
    "    return resized_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014012ae-7a46-493b-83dd-87669ed7f846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([106, 32, 32, 3])\n",
      "torch.Size([106, 4, 4])\n",
      "tensor(44.4444, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "images = data['images']\n",
    "poses = data['poses']\n",
    "focal_length = data['focal']\n",
    "\n",
    "# Images\n",
    "# (B, H, W, C)\n",
    "images = torch.from_numpy(images)\n",
    "images = resize_to(images, 32, 32)\n",
    "# Camera extrinsics (poses)\n",
    "poses = torch.from_numpy(poses)\n",
    "# Focal length (intrinsics)\n",
    "focal_length = torch.from_numpy(focal_length)\n",
    "# Rescale focal length\n",
    "focal_length = focal_length * 32.0 / 100.0\n",
    "\n",
    "print(images.shape)\n",
    "print(poses.shape)\n",
    "print(focal_length)\n",
    "\n",
    "height, width = images.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a344e-2055-48d1-be1c-40f22cfe4bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85983073-391c-406b-95a4-c46bb6d58d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716f93f-e73e-4af7-a003-609e3261e47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0a3533-2d18-487f-8673-c0af26020aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ray_bundle(height: int, width: int, focal_length: float, tform_cam2world: torch.Tensor):\n",
    "  r\"\"\"Compute the bundle of rays passing through all pixels of an image (one ray per pixel).\n",
    "\n",
    "  Args:\n",
    "    height (int): Height of an image (number of pixels).\n",
    "    width (int): Width of an image (number of pixels).\n",
    "    focal_length (float or torch.Tensor): Focal length (number of pixels, i.e., calibrated intrinsics).\n",
    "    tform_cam2world (torch.Tensor): A 6-DoF rigid-body transform (shape: :math:`(4, 4)`) that\n",
    "      transforms a 3D point from the camera frame to the \"world\" frame for the current example.\n",
    "  \n",
    "  Returns:\n",
    "    ray_origins (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the centers of\n",
    "      each ray. `ray_origins[i][j]` denotes the origin of the ray passing through pixel at\n",
    "      row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "    ray_directions (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the\n",
    "      direction of each ray (a unit vector). `ray_directions[i][j]` denotes the direction of the ray\n",
    "      passing through the pixel at row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "  \"\"\"\n",
    "  \n",
    "  ii, jj = torch.meshgrid(\n",
    "      torch.arange(width),\n",
    "      torch.arange(height),\n",
    "      indexing='xy'\n",
    "  )\n",
    "\n",
    "  # (H, W, 3)\n",
    "  directions = torch.stack(\n",
    "    [\n",
    "      (ii - width * .5) / focal_length,\n",
    "      -(jj - height * .5) / focal_length,\n",
    "      -torch.ones_like(ii)\n",
    "    ],\n",
    "    dim=-1\n",
    "  )\n",
    "\n",
    "  # ray_directions = torch.sum(directions[..., None, :] * tform_cam2world[:3, :3], dim=-1)\n",
    "\n",
    "  ray_directions = directions @ tform_cam2world[:3, :3].T\n",
    "  ray_origins = tform_cam2world[:3, -1].expand(ray_directions.shape)\n",
    "  return ray_origins, ray_directions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0c5d8a-5b95-4298-b69e-556b36cb26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_get_ray_bundle(\n",
    "    height: int,\n",
    "    width: int,\n",
    "    focal_length: torch.Tensor,\n",
    "    pose: torch.Tensor\n",
    "):\n",
    "    points_x, points_y = torch.meshgrid(\n",
    "        torch.arange(width),\n",
    "        torch.arange(height),\n",
    "        indexing='xy'\n",
    "    )\n",
    "\n",
    "    points_x = (points_x - width / 2.0) / focal_length\n",
    "    # Note the -ve here, y in grid increases downwards while\n",
    "    # y in NDC increases upwards\n",
    "    points_y = -(points_y - height / 2.0) / focal_length\n",
    "    points_z = -tr.ones_like(points_x)\n",
    "\n",
    "    ray_dirs = tr.stack(\n",
    "        (\n",
    "            points_x,\n",
    "            points_y,\n",
    "            points_z,\n",
    "        ),\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    transform_rot = pose[:3, :3]\n",
    "    ray_dirs = ray_dirs @ transform_rot.T\n",
    "\n",
    "    ray_origins = pose[:3, -1].expand(ray_dirs.shape)\n",
    "\n",
    "    return ray_origins, ray_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f70e084-6669-4302-9878-377f35a02aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ao, ad = nf_get_ray_bundle(height, width, focal_length, poses[0])\n",
    "bo, bd = get_ray_bundle(height, width, focal_length, poses[0])\n",
    "\n",
    "tr.all(ao == bo), tr.all(ad == bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "393e9d3d-5428-4b30-9a09-07a71acc6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_points_from_rays(\n",
    "    ray_origins: torch.Tensor,\n",
    "    ray_directions: torch.Tensor,\n",
    "    near_thresh: float,\n",
    "    far_thresh: float,\n",
    "    num_samples: int,\n",
    "    randomize = False\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "  r\"\"\"Compute query 3D points given the \"bundle\" of rays. The near_thresh and far_thresh\n",
    "  variables indicate the bounds within which 3D points are to be sampled.\n",
    "\n",
    "  Args:\n",
    "    ray_origins (torch.Tensor): Origin of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    ray_directions (torch.Tensor): Direction of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    near_thresh (float): The 'near' extent of the bounding volume (i.e., the nearest depth\n",
    "      coordinate that is of interest/relevance).\n",
    "    far_thresh (float): The 'far' extent of the bounding volume (i.e., the farthest depth\n",
    "      coordinate that is of interest/relevance).\n",
    "    num_samples (int): Number of samples to be drawn along each ray. Samples are drawn\n",
    "      randomly, whilst trying to ensure \"some form of\" uniform spacing among them.\n",
    "    randomize (optional, bool): Whether or not to randomize the sampling of query points.\n",
    "      By default, this is set to `True`. If disabled (by setting to `False`), we sample\n",
    "      uniformly spaced points along each ray in the \"bundle\".\n",
    "  \n",
    "  Returns:\n",
    "    query_points (torch.Tensor): Query points along each ray\n",
    "      (shape: :math:`(width, height, num_samples, 3)`).\n",
    "    depth_values (torch.Tensor): Sampled depth values along each ray\n",
    "      (shape: :math:`(num_samples)`).\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  # shape: (num_samples)\n",
    "  depth_values = torch.linspace(near_thresh, far_thresh, num_samples).to(ray_origins)\n",
    "  if randomize is True:\n",
    "    # ray_origins: (width, height, 3)\n",
    "    # noise_shape = (width, height, num_samples)\n",
    "    noise_shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
    "    # depth_values: (num_samples)\n",
    "    depth_values = depth_values \\\n",
    "        + torch.rand(noise_shape).to(ray_origins) * (far_thresh\n",
    "            - near_thresh) / num_samples\n",
    "  # (width, height, num_samples, 3) = (width, height, 1, 3) + (width, height, 1, 3) * (num_samples, 1)\n",
    "  # query_points:  (width, height, num_samples, 3)\n",
    "  query_points = ray_origins[..., None, :] + ray_directions[..., None, :] * depth_values[..., :, None]\n",
    "  # TODO: Double-check that `depth_values` returned is of shape `(num_samples)`.\n",
    "  return query_points, depth_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99a54862-3abe-468c-966a-80f55a0ff37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_create_query_points(\n",
    "    # (H, W, 3)\n",
    "    ray_origins: torch.Tensor,\n",
    "    # (H, W, 3)\n",
    "    ray_dirs: torch.Tensor,\n",
    "    thresh_near: float,\n",
    "    thresh_far: float,\n",
    "    num_samples_per_ray: int,\n",
    "):\n",
    "    # TODO: randomize\n",
    "\n",
    "    # (N,)\n",
    "    depths = torch.linspace(thresh_near, thresh_far, num_samples_per_ray)\n",
    "\n",
    "    # (H, W, N, 3)\n",
    "    query_points = (\n",
    "        ray_origins[..., None, :]\n",
    "        + ray_dirs[..., None, :] * depths[:, None]\n",
    "    )\n",
    "\n",
    "    return query_points, depths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea12e1e5-ad06-40e9-babf-98209c8f8182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aq, az = nf_create_query_points(ao, ad, 2, 6, 32)\n",
    "bq, bz = compute_query_points_from_rays(bo, bd, 2, 6, 32)\n",
    "\n",
    "tr.all(aq == bq), tr.all(az == bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "460d2777-445d-444b-a71e-37c0ca804076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    cumprod = torch.cumprod(tensor, dim=-1)\n",
    "    cumprod = torch.roll(cumprod, 1, dims=-1)\n",
    "    cumprod[..., 0] = 1.\n",
    "    return cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14f514e6-0e44-490d-bcad-a1f6562e361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_volume_density(\n",
    "    radiance_field: torch.Tensor,\n",
    "    depth_values: torch.Tensor\n",
    "):\n",
    "  r\"\"\"Differentiably renders a radiance field, given the origin of each ray in the\n",
    "  \"bundle\", and the sampled depth values along them.\n",
    "\n",
    "  Args:\n",
    "    radiance_field (torch.Tensor): A \"field\" where, at each query location (X, Y, Z),\n",
    "      we have an emitted (RGB) color and a volume density (denoted :math:`\\sigma` in\n",
    "      the paper) (shape: :math:`(width, height, num_samples, 4)`).\n",
    "    ray_origins (torch.Tensor): Origin of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    depth_values (torch.Tensor): Sampled depth values along each ray\n",
    "      (shape: :math:`(num_samples)`).\n",
    "  \n",
    "  Returns:\n",
    "    rgb_map (torch.Tensor): Rendered RGB image (shape: :math:`(width, height, 3)`).\n",
    "    depth_map (torch.Tensor): Rendered depth image (shape: :math:`(width, height)`).\n",
    "    acc_map (torch.Tensor): # TODO: Double-check (I think this is the accumulated\n",
    "      transmittance map).\n",
    "  \"\"\"\n",
    "\n",
    "  # TESTED\n",
    "\n",
    "  # radiance_field : (H, W, N, 4)\n",
    "  # depth_values : (H, W, N)\n",
    "\n",
    "  # (H, W, N, 3)\n",
    "  rgb = torch.sigmoid(radiance_field[..., :3])\n",
    "\n",
    "  # (H, W, N)\n",
    "  sigma_a = torch.nn.functional.relu(radiance_field[..., 3])\n",
    "\n",
    "  # (1,)\n",
    "  one_e_10 = torch.tensor([1e10])\n",
    "\n",
    "  # (H, W, N)\n",
    "  dists = torch.cat(\n",
    "    (\n",
    "      # (H, W, N - 1)\n",
    "      depth_values[..., 1:] - depth_values[..., :-1],\n",
    "      # (H, W, 1)\n",
    "      one_e_10.expand(depth_values[..., :1].shape)\n",
    "    ),\n",
    "    dim=-1\n",
    "  )\n",
    "\n",
    "  # (H, W, N)\n",
    "  alpha = 1. - torch.exp(-sigma_a * dists)\n",
    "  # (H, W, N)\n",
    "  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "  # (H, W, N, 3)\n",
    "  rgb_map_points = (\n",
    "    # (H, W, N, 1)\n",
    "    weights[..., None]\n",
    "    *\n",
    "    # (H, W, N, 3)\n",
    "    rgb\n",
    "  )\n",
    "\n",
    "  # (H, W, 3)\n",
    "  rgb_map = rgb_map_points.sum(dim=-2)\n",
    "\n",
    "  # depth_map = (weights * depth_values).sum(dim=-1)\n",
    "  # acc_map = weights.sum(-1)\n",
    "\n",
    "  return rgb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae306159-ba62-431a-8901-50c8a1c1fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_render_view(\n",
    "    # (H, W, N, 4)\n",
    "    view_field: torch.Tensor,\n",
    "    # (N,) or (H, W, N)\n",
    "    depths: torch.Tensor,\n",
    "):\n",
    "    # (H, W, N, 3)\n",
    "    rgb_field = view_field[..., :3]\n",
    "    # (H, W, N)\n",
    "    sigma_field = view_field[..., 3]\n",
    "\n",
    "    rgb_field = F.sigmoid(rgb_field)\n",
    "    sigma_field = F.relu(sigma_field)\n",
    "\n",
    "    # (*, N - 1)\n",
    "    deltas = depths[..., 1:] - depths[..., :-1]\n",
    "\n",
    "    # (*, N)\n",
    "    deltas = torch.cat(\n",
    "        (\n",
    "            # (*, N - 1)\n",
    "            deltas,\n",
    "            # (*, 1)\n",
    "            torch.tensor([1e10]).expand(deltas[..., :1].shape)\n",
    "        ),\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    # (H, W, N)\n",
    "    alpha = 1. - torch.exp(-sigma_field * deltas)\n",
    "    # (H, W, N)\n",
    "    weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "    # (H, W, N, 3)\n",
    "    rgb_map_points = (\n",
    "      # (H, W, N, 1)\n",
    "      weights[..., None]\n",
    "      *\n",
    "      # (H, W, N, 3)\n",
    "      rgb_field\n",
    "    )\n",
    "\n",
    "    # (H, W, 3)\n",
    "    rgb_map = rgb_map_points.sum(dim=-2)\n",
    "\n",
    "    return rgb_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a3949f-cfaa-4afc-837a-b30ca6871d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rad_field = tr.randn(height, width, 32, 4)\n",
    "\n",
    "argb = nf_render_view(rad_field, az)\n",
    "brgb = render_volume_density(rad_field, az)\n",
    "\n",
    "torch.all(argb == brgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6cff157-9d8e-4c0b-8913-05d03abf8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(\n",
    "    # (*, D (3))\n",
    "    points,\n",
    "    L=6,\n",
    "):\n",
    "    encoding = [points]\n",
    "\n",
    "    freqs = 2.0 ** torch.linspace(0.0, L - 1, L)\n",
    "\n",
    "    for freq in freqs:\n",
    "        encoding.append(torch.sin(points * freq))\n",
    "        encoding.append(torch.cos(points * freq))\n",
    "\n",
    "    if len(encoding) == 1:\n",
    "        return encoding[0]\n",
    "    else:\n",
    "        return torch.cat(encoding, dim=-1)\n",
    "\n",
    "\n",
    "def split_points_into_chunks(\n",
    "    # (B, L)\n",
    "    points: torch.Tensor,\n",
    "    chunk_size: int\n",
    "):\n",
    "    return [\n",
    "        points[i:i + chunk_size]\n",
    "        for i in range(0, points.shape[0], chunk_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef62d96e-3939-41ba-9f91-22dd2d1cd0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_render_pose(\n",
    "    model: torch.nn.Module,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    focal_length: int,\n",
    "    pose: torch.Tensor,\n",
    "    thresh_near: int,\n",
    "    thresh_far: int,\n",
    "    num_samples_per_ray: int,\n",
    "    chunk_size: int,\n",
    "):\n",
    "\n",
    "    # Create rays\n",
    "    ray_origins, ray_dirs = nf_get_ray_bundle(\n",
    "        height,\n",
    "        width,\n",
    "        focal_length,\n",
    "        pose\n",
    "    )\n",
    "\n",
    "    # Create query points\n",
    "    query_points, depths = nf_create_query_points(\n",
    "        ray_origins,\n",
    "        ray_dirs,\n",
    "        thresh_near,\n",
    "        thresh_far,\n",
    "        num_samples_per_ray,\n",
    "    )\n",
    "\n",
    "    # pass query points to model\n",
    "    \"\"\"\n",
    "    model: (B, 3) -> (B, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    # (H, W, N, 3)\n",
    "    # query_points\n",
    "\n",
    "    # (H*W*N, 3)\n",
    "    flat_query_points = query_points.view(-1, 3)\n",
    "\n",
    "    # apply positional encoding\n",
    "    flat_query_points = positional_encoding(flat_query_points)\n",
    "\n",
    "    # convert flat_query_points to chunks\n",
    "    chunks = split_points_into_chunks(\n",
    "        flat_query_points, chunk_size)\n",
    "    outputs = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # (Bi, 4)\n",
    "        chunk_view_field = model(chunk)\n",
    "        outputs.append(chunk_view_field)\n",
    "\n",
    "    # (H*W*N, 4)\n",
    "    flat_view_field = torch.cat(outputs, dim=0)\n",
    "\n",
    "    # create view (radiance field)\n",
    "    # (H, W, N, 4)\n",
    "    view_field = flat_view_field.view(\n",
    "        list(query_points.shape[:-1]) + [-1]\n",
    "    )\n",
    "\n",
    "    rgb_map = nf_render_view(\n",
    "        view_field,\n",
    "        depths   \n",
    "    )\n",
    "\n",
    "    return rgb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e73d78f0-8c11-4c00-b11a-a941332c04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryTinyNerfModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filter_size=128,\n",
    "        num_encoding_functions=6\n",
    "    ):\n",
    "\n",
    "        super(VeryTinyNerfModel, self).__init__()\n",
    "        # Input layer (default: 39 -> 128)\n",
    "        self.layer1 = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n",
    "        # Layer 2 (default: 128 -> 128)\n",
    "        self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "        # Layer 3 (default: 128 -> 4)\n",
    "        self.layer3 = torch.nn.Linear(filter_size, 4)\n",
    "        # Short hand for torch.nn.functional.relu\n",
    "        self.relu = torch.nn.functional.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62f28529-4ad9-4df1-a94a-a90567329707",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VeryTinyNerfModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b83d39a0-0614-4900-a836-df6ff60d3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pose: torch.Tensor):\n",
    "    return nf_render_pose(\n",
    "        model,\n",
    "        height,\n",
    "        width,\n",
    "        focal_length,\n",
    "        pose=pose,\n",
    "        thresh_near=2,\n",
    "        thresh_far=6,\n",
    "        num_samples_per_ray=32,\n",
    "        chunk_size=8096,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8386b3d3-e6c9-4307-ac94-f7e44b4df310",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c81d8cbe-9966-4094-be89-83fbb1c77cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.006026849616318941\n",
      "10: 0.0058042374439537525\n",
      "20: 0.003926482051610947\n",
      "30: 0.0037257529329508543\n",
      "40: 0.008596851490437984\n",
      "50: 0.005437550600618124\n",
      "60: 0.008384843356907368\n",
      "70: 0.0043747504241764545\n",
      "80: 0.004508476238697767\n",
      "90: 0.006997155491262674\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i in range(100):\n",
    "    idx = torch.randint(images.shape[0], (1,)).item()\n",
    "    target_pose = poses[idx]\n",
    "    # (H, W, 3)\n",
    "    target_image = images[idx]\n",
    "    \n",
    "    # (H, W, 3)\n",
    "    image_predicted = predict(target_pose)\n",
    "\n",
    "    loss = F.mse_loss(image_predicted, target_image)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i}: {loss.item()}\") \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "018891a9-4d3f-4bd9-9683-d19ed2ae06b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7995fad5f650>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_idx = torch.randint(images.shape[0], (1,)).item()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "predicted = predict(poses[test_idx]).detach()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(predicted)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(images[test_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
