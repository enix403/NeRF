{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8f22c2-aa8b-498b-8dfd-856b0600559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch as tr\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1dd1bb-c5d4-4b4f-a8b4-b60ebdb1f9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile 'tiny_nerf_data.npz' with keys: images, poses, focal"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input images, poses, and intrinsics\n",
    "data = np.load(\"tiny_nerf_data.npz\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c548dd4-2b8e-40e8-be73-3efb851ce1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to(images, height, width):\n",
    "    # images: (B, old_H, old_W, C)\n",
    "\n",
    "    # (B, C, old_H, old_W)\n",
    "    images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "    transform = tv.transforms.Compose([\n",
    "        tv.transforms.Resize((height, width))\n",
    "    ])\n",
    "\n",
    "    # (B, C, new_H, new_W)\n",
    "    resized_images = torch.stack([\n",
    "        # (C, new_H, new_W)\n",
    "        transform(image)\n",
    "        for image in images\n",
    "    ])\n",
    "\n",
    "    # (B, new_H, new_W, C)\n",
    "    resized_images = resized_images.permute(0, 2, 3, 1)\n",
    "\n",
    "    return resized_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014012ae-7a46-493b-83dd-87669ed7f846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([106, 32, 32, 3])\n",
      "torch.Size([106, 4, 4])\n",
      "tensor(44.4444, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "images = data['images']\n",
    "poses = data['poses']\n",
    "focal_length = data['focal']\n",
    "\n",
    "# Images\n",
    "# (B, H, W, C)\n",
    "images = torch.from_numpy(images)\n",
    "images = resize_to(images, 32, 32)\n",
    "# Camera extrinsics (poses)\n",
    "poses = torch.from_numpy(poses)\n",
    "# Focal length (intrinsics)\n",
    "focal_length = torch.from_numpy(focal_length)\n",
    "# Rescale focal length\n",
    "focal_length = focal_length * 32.0 / 100.0\n",
    "\n",
    "print(images.shape)\n",
    "print(poses.shape)\n",
    "print(focal_length)\n",
    "\n",
    "height, width = images.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a344e-2055-48d1-be1c-40f22cfe4bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85983073-391c-406b-95a4-c46bb6d58d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716f93f-e73e-4af7-a003-609e3261e47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0a3533-2d18-487f-8673-c0af26020aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ray_bundle(height: int, width: int, focal_length: float, tform_cam2world: torch.Tensor):\n",
    "  r\"\"\"Compute the bundle of rays passing through all pixels of an image (one ray per pixel).\n",
    "\n",
    "  Args:\n",
    "    height (int): Height of an image (number of pixels).\n",
    "    width (int): Width of an image (number of pixels).\n",
    "    focal_length (float or torch.Tensor): Focal length (number of pixels, i.e., calibrated intrinsics).\n",
    "    tform_cam2world (torch.Tensor): A 6-DoF rigid-body transform (shape: :math:`(4, 4)`) that\n",
    "      transforms a 3D point from the camera frame to the \"world\" frame for the current example.\n",
    "  \n",
    "  Returns:\n",
    "    ray_origins (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the centers of\n",
    "      each ray. `ray_origins[i][j]` denotes the origin of the ray passing through pixel at\n",
    "      row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "    ray_directions (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the\n",
    "      direction of each ray (a unit vector). `ray_directions[i][j]` denotes the direction of the ray\n",
    "      passing through the pixel at row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "  \"\"\"\n",
    "  \n",
    "  ii, jj = torch.meshgrid(\n",
    "      torch.arange(width),\n",
    "      torch.arange(height),\n",
    "      indexing='xy'\n",
    "  )\n",
    "\n",
    "  # (H, W, 3)\n",
    "  directions = torch.stack(\n",
    "    [\n",
    "      (ii - width * .5) / focal_length,\n",
    "      -(jj - height * .5) / focal_length,\n",
    "      -torch.ones_like(ii)\n",
    "    ],\n",
    "    dim=-1\n",
    "  )\n",
    "\n",
    "  # ray_directions = torch.sum(directions[..., None, :] * tform_cam2world[:3, :3], dim=-1)\n",
    "\n",
    "  ray_directions = directions @ tform_cam2world[:3, :3].T\n",
    "  ray_origins = tform_cam2world[:3, -1].expand(ray_directions.shape)\n",
    "  return ray_origins, ray_directions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0c5d8a-5b95-4298-b69e-556b36cb26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_get_ray_bundle(\n",
    "    height: int,\n",
    "    width: int,\n",
    "    focal_length: torch.Tensor,\n",
    "    pose: torch.Tensor\n",
    "):\n",
    "    points_x, points_y = torch.meshgrid(\n",
    "        torch.arange(width),\n",
    "        torch.arange(height),\n",
    "        indexing='xy'\n",
    "    )\n",
    "\n",
    "    points_x = (points_x - width / 2.0) / focal_length\n",
    "    # Note the -ve here, y in grid increases downwards while\n",
    "    # y in NDC increases upwards\n",
    "    points_y = -(points_y - height / 2.0) / focal_length\n",
    "    points_z = -tr.ones_like(points_x)\n",
    "\n",
    "    ray_dirs = tr.stack(\n",
    "        (\n",
    "            points_x,\n",
    "            points_y,\n",
    "            points_z,\n",
    "        ),\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    transform_rot = pose[:3, :3]\n",
    "    ray_dirs = ray_dirs @ transform_rot.T\n",
    "\n",
    "    ray_origins = pose[:3, -1].expand(ray_dirs.shape)\n",
    "\n",
    "    return ray_origins, ray_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f70e084-6669-4302-9878-377f35a02aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ao, ad = nf_get_ray_bundle(height, width, focal_length, poses[0])\n",
    "bo, bd = get_ray_bundle(height, width, focal_length, poses[0])\n",
    "\n",
    "tr.all(ao == bo), tr.all(ad == bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "393e9d3d-5428-4b30-9a09-07a71acc6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_points_from_rays(\n",
    "    ray_origins: torch.Tensor,\n",
    "    ray_directions: torch.Tensor,\n",
    "    near_thresh: float,\n",
    "    far_thresh: float,\n",
    "    num_samples: int,\n",
    "    randomize = False\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "  r\"\"\"Compute query 3D points given the \"bundle\" of rays. The near_thresh and far_thresh\n",
    "  variables indicate the bounds within which 3D points are to be sampled.\n",
    "\n",
    "  Args:\n",
    "    ray_origins (torch.Tensor): Origin of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    ray_directions (torch.Tensor): Direction of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    near_thresh (float): The 'near' extent of the bounding volume (i.e., the nearest depth\n",
    "      coordinate that is of interest/relevance).\n",
    "    far_thresh (float): The 'far' extent of the bounding volume (i.e., the farthest depth\n",
    "      coordinate that is of interest/relevance).\n",
    "    num_samples (int): Number of samples to be drawn along each ray. Samples are drawn\n",
    "      randomly, whilst trying to ensure \"some form of\" uniform spacing among them.\n",
    "    randomize (optional, bool): Whether or not to randomize the sampling of query points.\n",
    "      By default, this is set to `True`. If disabled (by setting to `False`), we sample\n",
    "      uniformly spaced points along each ray in the \"bundle\".\n",
    "  \n",
    "  Returns:\n",
    "    query_points (torch.Tensor): Query points along each ray\n",
    "      (shape: :math:`(width, height, num_samples, 3)`).\n",
    "    depth_values (torch.Tensor): Sampled depth values along each ray\n",
    "      (shape: :math:`(num_samples)`).\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  # shape: (num_samples)\n",
    "  depth_values = torch.linspace(near_thresh, far_thresh, num_samples).to(ray_origins)\n",
    "  if randomize is True:\n",
    "    # ray_origins: (width, height, 3)\n",
    "    # noise_shape = (width, height, num_samples)\n",
    "    noise_shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
    "    # depth_values: (num_samples)\n",
    "    depth_values = depth_values \\\n",
    "        + torch.rand(noise_shape).to(ray_origins) * (far_thresh\n",
    "            - near_thresh) / num_samples\n",
    "  # (width, height, num_samples, 3) = (width, height, 1, 3) + (width, height, 1, 3) * (num_samples, 1)\n",
    "  # query_points:  (width, height, num_samples, 3)\n",
    "  query_points = ray_origins[..., None, :] + ray_directions[..., None, :] * depth_values[..., :, None]\n",
    "  # TODO: Double-check that `depth_values` returned is of shape `(num_samples)`.\n",
    "  return query_points, depth_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99a54862-3abe-468c-966a-80f55a0ff37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_create_query_points(\n",
    "    # (H, W, 3)\n",
    "    ray_origins: torch.Tensor,\n",
    "    # (H, W, 3)\n",
    "    ray_dirs: torch.Tensor,\n",
    "    thresh_near: float,\n",
    "    thresh_far: float,\n",
    "    num_samples_per_ray: int,\n",
    "):\n",
    "    # TODO: randomize\n",
    "\n",
    "    # (N,)\n",
    "    depths = torch.linspace(thresh_near, thresh_far, num_samples_per_ray)\n",
    "\n",
    "    # (H, W, N, 3)\n",
    "    query_points = (\n",
    "        ray_origins[..., None, :]\n",
    "        + ray_dirs[..., None, :] * depths[:, None]\n",
    "    )\n",
    "\n",
    "    return query_points, depths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea12e1e5-ad06-40e9-babf-98209c8f8182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aq, az = nf_create_query_points(ao, ad, 2, 6, 32)\n",
    "bq, bz = compute_query_points_from_rays(bo, bd, 2, 6, 32)\n",
    "\n",
    "tr.all(aq == bq), tr.all(az == bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "460d2777-445d-444b-a71e-37c0ca804076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    cumprod = torch.cumprod(tensor, dim=-1)\n",
    "    cumprod = torch.roll(cumprod, 1, dims=-1)\n",
    "    cumprod[..., 0] = 1.\n",
    "    return cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14f514e6-0e44-490d-bcad-a1f6562e361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_volume_density(\n",
    "    radiance_field: torch.Tensor,\n",
    "    depth_values: torch.Tensor\n",
    "):\n",
    "  r\"\"\"Differentiably renders a radiance field, given the origin of each ray in the\n",
    "  \"bundle\", and the sampled depth values along them.\n",
    "\n",
    "  Args:\n",
    "    radiance_field (torch.Tensor): A \"field\" where, at each query location (X, Y, Z),\n",
    "      we have an emitted (RGB) color and a volume density (denoted :math:`\\sigma` in\n",
    "      the paper) (shape: :math:`(width, height, num_samples, 4)`).\n",
    "    ray_origins (torch.Tensor): Origin of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    depth_values (torch.Tensor): Sampled depth values along each ray\n",
    "      (shape: :math:`(num_samples)`).\n",
    "  \n",
    "  Returns:\n",
    "    rgb_map (torch.Tensor): Rendered RGB image (shape: :math:`(width, height, 3)`).\n",
    "    depth_map (torch.Tensor): Rendered depth image (shape: :math:`(width, height)`).\n",
    "    acc_map (torch.Tensor): # TODO: Double-check (I think this is the accumulated\n",
    "      transmittance map).\n",
    "  \"\"\"\n",
    "\n",
    "  # TESTED\n",
    "\n",
    "  # radiance_field : (H, W, N, 4)\n",
    "  # depth_values : (H, W, N)\n",
    "\n",
    "  # (H, W, N, 3)\n",
    "  rgb = torch.sigmoid(radiance_field[..., :3])\n",
    "\n",
    "  # (H, W, N)\n",
    "  sigma_a = torch.nn.functional.relu(radiance_field[..., 3])\n",
    "\n",
    "  # (1,)\n",
    "  one_e_10 = torch.tensor([1e10])\n",
    "\n",
    "  # (H, W, N)\n",
    "  dists = torch.cat(\n",
    "    (\n",
    "      # (H, W, N - 1)\n",
    "      depth_values[..., 1:] - depth_values[..., :-1],\n",
    "      # (H, W, 1)\n",
    "      one_e_10.expand(depth_values[..., :1].shape)\n",
    "    ),\n",
    "    dim=-1\n",
    "  )\n",
    "\n",
    "  # (H, W, N)\n",
    "  alpha = 1. - torch.exp(-sigma_a * dists)\n",
    "  # (H, W, N)\n",
    "  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "  # (H, W, N, 3)\n",
    "  rgb_map_points = (\n",
    "    # (H, W, N, 1)\n",
    "    weights[..., None]\n",
    "    *\n",
    "    # (H, W, N, 3)\n",
    "    rgb\n",
    "  )\n",
    "\n",
    "  # (H, W, 3)\n",
    "  rgb_map = rgb_map_points.sum(dim=-2)\n",
    "\n",
    "  # depth_map = (weights * depth_values).sum(dim=-1)\n",
    "  # acc_map = weights.sum(-1)\n",
    "\n",
    "  return rgb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae306159-ba62-431a-8901-50c8a1c1fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_render_view(\n",
    "    # (H, W, N, 4)\n",
    "    view_field: torch.Tensor,\n",
    "    # (N,) or (H, W, N)\n",
    "    depths: torch.Tensor,\n",
    "):\n",
    "    # (H, W, N, 3)\n",
    "    rgb_field = view_field[..., :3]\n",
    "    # (H, W, N)\n",
    "    sigma_field = view_field[..., 3]\n",
    "\n",
    "    rgb_field = F.sigmoid(rgb_field)\n",
    "    sigma_field = F.relu(sigma_field)\n",
    "\n",
    "    # (*, N - 1)\n",
    "    deltas = depths[..., 1:] - depths[..., :-1]\n",
    "\n",
    "    # (*, N)\n",
    "    deltas = torch.cat(\n",
    "        (\n",
    "            # (*, N - 1)\n",
    "            deltas,\n",
    "            # (*, 1)\n",
    "            torch.tensor([1e10]).expand(deltas[..., :1].shape)\n",
    "        ),\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    # (H, W, N)\n",
    "    alpha = 1. - torch.exp(-sigma_field * deltas)\n",
    "    # (H, W, N)\n",
    "    weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "    # (H, W, N, 3)\n",
    "    rgb_map_points = (\n",
    "      # (H, W, N, 1)\n",
    "      weights[..., None]\n",
    "      *\n",
    "      # (H, W, N, 3)\n",
    "      rgb_field\n",
    "    )\n",
    "\n",
    "    # (H, W, 3)\n",
    "    rgb_map = rgb_map_points.sum(dim=-2)\n",
    "\n",
    "    return rgb_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a3949f-cfaa-4afc-837a-b30ca6871d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rad_field = tr.randn(height, width, 32, 4)\n",
    "\n",
    "argb = nf_render_view(rad_field, az)\n",
    "brgb = render_volume_density(rad_field, az)\n",
    "\n",
    "torch.all(argb == brgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6cff157-9d8e-4c0b-8913-05d03abf8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(\n",
    "    # (*, D (3))\n",
    "    points,\n",
    "    L=6,\n",
    "):\n",
    "    encoding = [points]\n",
    "\n",
    "    freqs = 2.0 ** torch.linspace(0.0, L - 1, L)\n",
    "\n",
    "    for freq in freqs:\n",
    "        encoding.append(torch.sin(points * freq))\n",
    "        encoding.append(torch.cos(points * freq))\n",
    "\n",
    "    if len(encoding) == 1:\n",
    "        return encoding[0]\n",
    "    else:\n",
    "        return torch.cat(encoding, dim=-1)\n",
    "\n",
    "\n",
    "def split_points_into_chunks(\n",
    "    # (B, L)\n",
    "    points: torch.Tensor,\n",
    "    chunk_size: int\n",
    "):\n",
    "    return [\n",
    "        points[i:i + chunk_size]\n",
    "        for i in range(0, points.shape[0], chunk_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef62d96e-3939-41ba-9f91-22dd2d1cd0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_render_pose(\n",
    "    model: torch.nn.Module,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    focal_length: int,\n",
    "    pose: torch.Tensor,\n",
    "    thresh_near: int,\n",
    "    thresh_far: int,\n",
    "    num_samples_per_ray: int,\n",
    "    chunk_size: int,\n",
    "):\n",
    "\n",
    "    # Create rays\n",
    "    ray_origins, ray_dirs = nf_get_ray_bundle(\n",
    "        height,\n",
    "        width,\n",
    "        focal_length,\n",
    "        pose\n",
    "    )\n",
    "\n",
    "    # Create query points\n",
    "    query_points, depths = nf_create_query_points(\n",
    "        ray_origins,\n",
    "        ray_dirs,\n",
    "        thresh_near,\n",
    "        thresh_far,\n",
    "        num_samples_per_ray,\n",
    "    )\n",
    "\n",
    "    # pass query points to model\n",
    "    \"\"\"\n",
    "    model: (B, 3) -> (B, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    # (H, W, N, 3)\n",
    "    # query_points\n",
    "\n",
    "    # (H*W*N, 3)\n",
    "    flat_query_points = query_points.view(-1, 3)\n",
    "\n",
    "    # apply positional encoding\n",
    "    flat_query_points = positional_encoding(flat_query_points)\n",
    "\n",
    "    # convert flat_query_points to chunks\n",
    "    chunks = split_points_into_chunks(\n",
    "        flat_query_points, chunk_size)\n",
    "    outputs = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # (Bi, 4)\n",
    "        chunk_view_field = model(chunk)\n",
    "        outputs.append(chunk_view_field)\n",
    "\n",
    "    # (H*W*N, 4)\n",
    "    flat_view_field = torch.cat(outputs, dim=0)\n",
    "\n",
    "    # create view (radiance field)\n",
    "    # (H, W, N, 4)\n",
    "    view_field = flat_view_field.view(\n",
    "        list(query_points.shape[:-1]) + [-1]\n",
    "    )\n",
    "\n",
    "    rgb_map = nf_render_view(\n",
    "        view_field,\n",
    "        depths   \n",
    "    )\n",
    "\n",
    "    return rgb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e73d78f0-8c11-4c00-b11a-a941332c04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryTinyNerfModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filter_size=128,\n",
    "        num_encoding_functions=6\n",
    "    ):\n",
    "\n",
    "        super(VeryTinyNerfModel, self).__init__()\n",
    "        # Input layer (default: 39 -> 128)\n",
    "        self.layer1 = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n",
    "        # Layer 2 (default: 128 -> 128)\n",
    "        self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "        # Layer 3 (default: 128 -> 4)\n",
    "        self.layer3 = torch.nn.Linear(filter_size, 4)\n",
    "        # Short hand for torch.nn.functional.relu\n",
    "        self.relu = torch.nn.functional.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62f28529-4ad9-4df1-a94a-a90567329707",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VeryTinyNerfModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7bf12af-e676-42f6-ba27-bc578700b938",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.Size' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m rgb_map \u001b[38;5;241m=\u001b[39m nf_render_pose(\n\u001b[1;32m      2\u001b[0m     model,\n\u001b[1;32m      3\u001b[0m     height,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mrgb_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
     ]
    }
   ],
   "source": [
    "rgb_map = nf_render_pose(\n",
    "    model,\n",
    "    height,\n",
    "    width,\n",
    "    focal_length,\n",
    "    pose=poses[0],\n",
    "    thresh_near=2,\n",
    "    thresh_far=6,\n",
    "    num_samples_per_ray=32,\n",
    "    chunk_size=4096,\n",
    ")\n",
    "\n",
    "rgb_map.shape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
