{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef8f22c2-aa8b-498b-8dfd-856b0600559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch as tr\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1dd1bb-c5d4-4b4f-a8b4-b60ebdb1f9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile 'tiny_nerf_data.npz' with keys: images, poses, focal"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input images, poses, and intrinsics\n",
    "data = np.load(\"tiny_nerf_data.npz\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c548dd4-2b8e-40e8-be73-3efb851ce1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to(images, height, width):\n",
    "    # images: (B, old_H, old_W, C)\n",
    "\n",
    "    # (B, C, old_H, old_W)\n",
    "    images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "    transform = tv.transforms.Compose([\n",
    "        tv.transforms.Resize((height, width))\n",
    "    ])\n",
    "\n",
    "    # (B, C, new_H, new_W)\n",
    "    resized_images = torch.stack([\n",
    "        # (C, new_H, new_W)\n",
    "        transform(image)\n",
    "        for image in images\n",
    "    ])\n",
    "\n",
    "    # (B, new_H, new_W, C)\n",
    "    resized_images = resized_images.permute(0, 2, 3, 1)\n",
    "\n",
    "    return resized_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "014012ae-7a46-493b-83dd-87669ed7f846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([106, 32, 32, 3])\n",
      "torch.Size([106, 4, 4])\n",
      "tensor(44.4444, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "images = data['images']\n",
    "poses = data['poses']\n",
    "focal_length = data['focal']\n",
    "\n",
    "# Images\n",
    "# (B, H, W, C)\n",
    "images = torch.from_numpy(images)\n",
    "images = resize_to(images, 32, 32)\n",
    "# Camera extrinsics (poses)\n",
    "poses = torch.from_numpy(poses)\n",
    "# Focal length (intrinsics)\n",
    "focal_length = torch.from_numpy(focal_length)\n",
    "# Rescale focal length\n",
    "focal_length = focal_length * 32.0 / 100.0\n",
    "\n",
    "print(images.shape)\n",
    "print(poses.shape)\n",
    "print(focal_length)\n",
    "\n",
    "height, width = images.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a344e-2055-48d1-be1c-40f22cfe4bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85983073-391c-406b-95a4-c46bb6d58d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716f93f-e73e-4af7-a003-609e3261e47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e0a3533-2d18-487f-8673-c0af26020aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ray_bundle(height: int, width: int, focal_length: float, tform_cam2world: torch.Tensor):\n",
    "  r\"\"\"Compute the bundle of rays passing through all pixels of an image (one ray per pixel).\n",
    "\n",
    "  Args:\n",
    "    height (int): Height of an image (number of pixels).\n",
    "    width (int): Width of an image (number of pixels).\n",
    "    focal_length (float or torch.Tensor): Focal length (number of pixels, i.e., calibrated intrinsics).\n",
    "    tform_cam2world (torch.Tensor): A 6-DoF rigid-body transform (shape: :math:`(4, 4)`) that\n",
    "      transforms a 3D point from the camera frame to the \"world\" frame for the current example.\n",
    "  \n",
    "  Returns:\n",
    "    ray_origins (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the centers of\n",
    "      each ray. `ray_origins[i][j]` denotes the origin of the ray passing through pixel at\n",
    "      row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "    ray_directions (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the\n",
    "      direction of each ray (a unit vector). `ray_directions[i][j]` denotes the direction of the ray\n",
    "      passing through the pixel at row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "  \"\"\"\n",
    "  \n",
    "  ii, jj = torch.meshgrid(\n",
    "      torch.arange(width),\n",
    "      torch.arange(height),\n",
    "      indexing='xy'\n",
    "  )\n",
    "\n",
    "  # (H, W, 3)\n",
    "  directions = torch.stack(\n",
    "    [\n",
    "      (ii - width * .5) / focal_length,\n",
    "      -(jj - height * .5) / focal_length,\n",
    "      -torch.ones_like(ii)\n",
    "    ],\n",
    "    dim=-1\n",
    "  )\n",
    "\n",
    "  # ray_directions = torch.sum(directions[..., None, :] * tform_cam2world[:3, :3], dim=-1)\n",
    "\n",
    "  ray_directions = directions @ tform_cam2world[:3, :3].T\n",
    "  ray_origins = tform_cam2world[:3, -1].expand(ray_directions.shape)\n",
    "  return ray_origins, ray_directions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d0c5d8a-5b95-4298-b69e-556b36cb26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ray_bundle_v2(\n",
    "    height: int,\n",
    "    width: int,\n",
    "    focal_length: torch.Tensor,\n",
    "    pose: torch.Tensor\n",
    "):\n",
    "    points_x, points_y = torch.meshgrid(\n",
    "        torch.arange(width),\n",
    "        torch.arange(height),\n",
    "        indexing='xy'\n",
    "    )\n",
    "\n",
    "    points_x = (points_x - width / 2.0) / focal_length\n",
    "    points_y = -(points_y - height / 2.0) / focal_length\n",
    "    points_z = -tr.ones_like(points_x)\n",
    "\n",
    "    ray_directions = tr.stack(\n",
    "        (\n",
    "            points_x,\n",
    "            points_y,\n",
    "            points_z,\n",
    "        ),\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    transform_rot = pose[:3, :3]\n",
    "    ray_directions = ray_directions @ transform_rot.T\n",
    "\n",
    "    ray_origins = pose[:3, -1].expand(ray_directions.shape)\n",
    "\n",
    "    return ray_origins, ray_directions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f70e084-6669-4302-9878-377f35a02aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ao, ad = get_ray_bundle_v2(height, width, focal_length, poses[0])\n",
    "bo, bd = get_ray_bundle(height, width, focal_length, poses[0])\n",
    "\n",
    "tr.all(ao == bo), tr.all(ad == bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "393e9d3d-5428-4b30-9a09-07a71acc6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_points_from_rays(\n",
    "    ray_origins: torch.Tensor,\n",
    "    ray_directions: torch.Tensor,\n",
    "    near_thresh: float,\n",
    "    far_thresh: float,\n",
    "    num_samples: int,\n",
    "    randomize = False\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "  r\"\"\"Compute query 3D points given the \"bundle\" of rays. The near_thresh and far_thresh\n",
    "  variables indicate the bounds within which 3D points are to be sampled.\n",
    "\n",
    "  Args:\n",
    "    ray_origins (torch.Tensor): Origin of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    ray_directions (torch.Tensor): Direction of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    near_thresh (float): The 'near' extent of the bounding volume (i.e., the nearest depth\n",
    "      coordinate that is of interest/relevance).\n",
    "    far_thresh (float): The 'far' extent of the bounding volume (i.e., the farthest depth\n",
    "      coordinate that is of interest/relevance).\n",
    "    num_samples (int): Number of samples to be drawn along each ray. Samples are drawn\n",
    "      randomly, whilst trying to ensure \"some form of\" uniform spacing among them.\n",
    "    randomize (optional, bool): Whether or not to randomize the sampling of query points.\n",
    "      By default, this is set to `True`. If disabled (by setting to `False`), we sample\n",
    "      uniformly spaced points along each ray in the \"bundle\".\n",
    "  \n",
    "  Returns:\n",
    "    query_points (torch.Tensor): Query points along each ray\n",
    "      (shape: :math:`(width, height, num_samples, 3)`).\n",
    "    depth_values (torch.Tensor): Sampled depth values along each ray\n",
    "      (shape: :math:`(num_samples)`).\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  # shape: (num_samples)\n",
    "  depth_values = torch.linspace(near_thresh, far_thresh, num_samples).to(ray_origins)\n",
    "  if randomize is True:\n",
    "    # ray_origins: (width, height, 3)\n",
    "    # noise_shape = (width, height, num_samples)\n",
    "    noise_shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
    "    # depth_values: (num_samples)\n",
    "    depth_values = depth_values \\\n",
    "        + torch.rand(noise_shape).to(ray_origins) * (far_thresh\n",
    "            - near_thresh) / num_samples\n",
    "  # (width, height, num_samples, 3) = (width, height, 1, 3) + (width, height, 1, 3) * (num_samples, 1)\n",
    "  # query_points:  (width, height, num_samples, 3)\n",
    "  query_points = ray_origins[..., None, :] + ray_directions[..., None, :] * depth_values[..., :, None]\n",
    "  # TODO: Double-check that `depth_values` returned is of shape `(num_samples)`.\n",
    "  return query_points, depth_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99a54862-3abe-468c-966a-80f55a0ff37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_create_query_points(\n",
    "    # (H, W, 3)\n",
    "    ray_origins: torch.Tensor,\n",
    "    # (H, W, 3)\n",
    "    ray_dirs: torch.Tensor,\n",
    "    thresh_near: float,\n",
    "    thresh_far: float,\n",
    "    num_samples_per_ray: int,\n",
    "):\n",
    "    # TODO: randomize\n",
    "\n",
    "    # (N,)\n",
    "    depths = torch.linspace(thresh_near, thresh_far, num_samples_per_ray)\n",
    "\n",
    "    # (H, W, N, 3)\n",
    "    query_points = (\n",
    "        ray_origins[..., None, :]\n",
    "        + ray_dirs[..., None, :] * depths[:, None]\n",
    "    )\n",
    "\n",
    "    return query_points, depths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea12e1e5-ad06-40e9-babf-98209c8f8182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aq, az = nf_create_query_points(ao, ad, 2, 6, 32)\n",
    "bq, bz = compute_query_points_from_rays(bo, bd, 2, 6, 32)\n",
    "\n",
    "tr.all(aq == bq), tr.all(az == bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "460d2777-445d-444b-a71e-37c0ca804076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    cumprod = torch.cumprod(tensor, dim=-1)\n",
    "    cumprod = torch.roll(cumprod, 1, dims=-1)\n",
    "    cumprod[..., 0] = 1.\n",
    "    return cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14f514e6-0e44-490d-bcad-a1f6562e361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_volume_density(\n",
    "    radiance_field: torch.Tensor,\n",
    "    depth_values: torch.Tensor\n",
    "):\n",
    "  r\"\"\"Differentiably renders a radiance field, given the origin of each ray in the\n",
    "  \"bundle\", and the sampled depth values along them.\n",
    "\n",
    "  Args:\n",
    "    radiance_field (torch.Tensor): A \"field\" where, at each query location (X, Y, Z),\n",
    "      we have an emitted (RGB) color and a volume density (denoted :math:`\\sigma` in\n",
    "      the paper) (shape: :math:`(width, height, num_samples, 4)`).\n",
    "    ray_origins (torch.Tensor): Origin of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    depth_values (torch.Tensor): Sampled depth values along each ray\n",
    "      (shape: :math:`(num_samples)`).\n",
    "  \n",
    "  Returns:\n",
    "    rgb_map (torch.Tensor): Rendered RGB image (shape: :math:`(width, height, 3)`).\n",
    "    depth_map (torch.Tensor): Rendered depth image (shape: :math:`(width, height)`).\n",
    "    acc_map (torch.Tensor): # TODO: Double-check (I think this is the accumulated\n",
    "      transmittance map).\n",
    "  \"\"\"\n",
    "\n",
    "  # TESTED\n",
    "\n",
    "  # radiance_field : (H, W, N, 4)\n",
    "  # depth_values : (H, W, N)\n",
    "\n",
    "  # (H, W, N, 3)\n",
    "  rgb = torch.sigmoid(radiance_field[..., :3])\n",
    "\n",
    "  # (H, W, N)\n",
    "  sigma_a = torch.nn.functional.relu(radiance_field[..., 3])\n",
    "\n",
    "  # (1,)\n",
    "  one_e_10 = torch.tensor([1e10])\n",
    "\n",
    "  # (H, W, N)\n",
    "  dists = torch.cat(\n",
    "    (\n",
    "      # (H, W, N - 1)\n",
    "      depth_values[..., 1:] - depth_values[..., :-1],\n",
    "      # (H, W, 1)\n",
    "      one_e_10.expand(depth_values[..., :1].shape)\n",
    "    ),\n",
    "    dim=-1\n",
    "  )\n",
    "\n",
    "  # (H, W, N)\n",
    "  alpha = 1. - torch.exp(-sigma_a * dists)\n",
    "  # (H, W, N)\n",
    "  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "  # (H, W, N, 3)\n",
    "  rgb_map_points = (\n",
    "    # (H, W, N, 1)\n",
    "    weights[..., None]\n",
    "    *\n",
    "    # (H, W, N, 3)\n",
    "    rgb\n",
    "  )\n",
    "\n",
    "  # (H, W, 3)\n",
    "  rgb_map = rgb_map_points.sum(dim=-2)\n",
    "\n",
    "  # depth_map = (weights * depth_values).sum(dim=-1)\n",
    "  # acc_map = weights.sum(-1)\n",
    "\n",
    "  return rgb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae306159-ba62-431a-8901-50c8a1c1fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_render_view(\n",
    "    # (H, W, N, 4)\n",
    "    view_field: torch.Tensor,\n",
    "    # (N,) or (H, W, N)\n",
    "    depths: torch.Tensor,\n",
    "):\n",
    "    # (H, W, N, 3)\n",
    "    rgb_field = view_field[..., :3]\n",
    "    # (H, W, N)\n",
    "    sigma_field = view_field[..., 3]\n",
    "\n",
    "    rgb_field = F.sigmoid(rgb_field)\n",
    "    sigma_field = F.relu(sigma_field)\n",
    "\n",
    "    # (*, N - 1)\n",
    "    deltas = depths[..., 1:] - depths[..., :1]\n",
    "\n",
    "    # (*, N)\n",
    "    deltas = torch.cat(\n",
    "        (\n",
    "            # (*, N - 1)\n",
    "            deltas,\n",
    "            # (*, 1)\n",
    "            torch.tensor([1e10]).expand(deltas[..., :1].shape)\n",
    "        ),\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    # (H, W, N)\n",
    "    # deltas = deltas.reshape(sigma_field.shape)\n",
    "\n",
    "    # (H, W, N)\n",
    "    alpha = 1. - torch.exp(-sigma_field * deltas)\n",
    "    # (H, W, N)\n",
    "    weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "    # (H, W, N, 3)\n",
    "    rgb_map_points = (\n",
    "      # (H, W, N, 1)\n",
    "      weights[..., None]\n",
    "      *\n",
    "      # (H, W, N, 3)\n",
    "      rgb_field\n",
    "    )\n",
    "\n",
    "    # (H, W, 3)\n",
    "    rgb_map = rgb_map_points.sum(dim=-2)\n",
    "\n",
    "    return rgb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9a3949f-cfaa-4afc-837a-b30ca6871d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4230, 0.4170, 0.4006],\n",
       "          [0.3154, 0.3377, 0.3621],\n",
       "          [0.4310, 0.4639, 0.5302],\n",
       "          ...,\n",
       "          [0.4263, 0.3634, 0.4302],\n",
       "          [0.4448, 0.3785, 0.3699],\n",
       "          [0.5275, 0.4371, 0.5298]],\n",
       " \n",
       "         [[0.2980, 0.5139, 0.4295],\n",
       "          [0.4746, 0.5171, 0.4456],\n",
       "          [0.5003, 0.4222, 0.4867],\n",
       "          ...,\n",
       "          [0.4264, 0.3635, 0.3657],\n",
       "          [0.4644, 0.4771, 0.5725],\n",
       "          [0.5276, 0.4771, 0.5564]],\n",
       " \n",
       "         [[0.5357, 0.5167, 0.5534],\n",
       "          [0.5879, 0.4118, 0.4341],\n",
       "          [0.3882, 0.3730, 0.4345],\n",
       "          ...,\n",
       "          [0.3135, 0.2775, 0.3060],\n",
       "          [0.4480, 0.2699, 0.2950],\n",
       "          [0.4294, 0.4076, 0.3626]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.2587, 0.2814, 0.2277],\n",
       "          [0.3327, 0.3398, 0.3191],\n",
       "          [0.4143, 0.4569, 0.5175],\n",
       "          ...,\n",
       "          [0.5159, 0.3236, 0.5347],\n",
       "          [0.3850, 0.4001, 0.3682],\n",
       "          [0.4583, 0.5859, 0.4631]],\n",
       " \n",
       "         [[0.4752, 0.3184, 0.4514],\n",
       "          [0.5638, 0.4306, 0.4647],\n",
       "          [0.5201, 0.5555, 0.5976],\n",
       "          ...,\n",
       "          [0.2656, 0.3989, 0.4463],\n",
       "          [0.3284, 0.3650, 0.2600],\n",
       "          [0.3556, 0.4556, 0.4175]],\n",
       " \n",
       "         [[0.4425, 0.3795, 0.3841],\n",
       "          [0.4151, 0.4301, 0.4271],\n",
       "          [0.4814, 0.5278, 0.4339],\n",
       "          ...,\n",
       "          [0.4065, 0.4383, 0.3612],\n",
       "          [0.3796, 0.4257, 0.3691],\n",
       "          [0.3414, 0.4563, 0.4244]]]),\n",
       " tensor([[2.4264, 2.5776, 2.6270,  ..., 2.7643, 2.8683, 3.8828],\n",
       "         [2.8327, 4.0528, 4.2737,  ..., 2.7704, 3.7379, 3.9439],\n",
       "         [3.2854, 3.1643, 2.6867,  ..., 3.0223, 2.2091, 3.2935],\n",
       "         ...,\n",
       "         [2.7477, 2.9294, 4.4749,  ..., 4.3496, 2.7088, 4.2400],\n",
       "         [3.2351, 3.1770, 3.9640,  ..., 3.0089, 2.4685, 2.4136],\n",
       "         [2.2450, 2.9201, 3.9533,  ..., 2.6122, 2.9604, 2.8907]]),\n",
       " tensor([[0.8400, 0.6834, 0.8897,  ..., 0.7433, 0.8652, 1.0000],\n",
       "         [0.8601, 1.0000, 1.0000,  ..., 0.7610, 1.0000, 1.0000],\n",
       "         [1.0000, 0.8511, 0.7843,  ..., 0.6781, 0.7316, 0.8052],\n",
       "         ...,\n",
       "         [0.6164, 0.6928, 1.0000,  ..., 1.0000, 0.7137, 1.0000],\n",
       "         [0.8252, 1.0000, 1.0000,  ..., 0.7366, 0.6654, 0.7473],\n",
       "         [0.7506, 0.7572, 1.0000,  ..., 0.7284, 0.8366, 0.8120]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rad_field = tr.randn(height, width, 32, 4)\n",
    "\n",
    "# nf_render_view(rad_field, az)\n",
    "render_volume_density(rad_field, az)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
